{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Conv_disc.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"FgsEDSxhbacE"},"source":["# Tensorflow\n","\n","*This worksheet was originally designed by [Erin George](https://www.math.ucla.edu/~egeo/) (Department of Mathematics, UCLA). It has been subsequently revised by later TAs and instructors.*\n","\n","Today, we're going to get some practice with Tensorflow.  This discussion is split in two parts.  In the first part, we're going to design a neural network together to solve a problem.  In the second part, we're going to poke at our neural network to see how we can improve it."]},{"cell_type":"markdown","metadata":{"id":"F0Ax-IlIbacc"},"source":["## Our Problem\n","\n","We will be working with the the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset again!"]},{"cell_type":"code","metadata":{"id":"9geT0ruybacd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636498943155,"user_tz":480,"elapsed":3257,"user":{"displayName":"JOHN ZHANG","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08103289824479183812"}},"outputId":"56d729be-0432-4422-9a35-f16f7b81a224"},"source":["import tensorflow as tf\n","import tensorflow.keras as keras\n","import numpy as np\n","\n","(x_train, y_train), (x_test, y_test) = \\\n","    tf.keras.datasets.mnist.load_data()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"markdown","metadata":{"id":"7urG7FNJbacg"},"source":["Again, I like to look at the data set first.  Let's choose one of the numbers:"]},{"cell_type":"code","metadata":{"id":"MQiI4lhXbach"},"source":["import matplotlib.pyplot as plt\n","plt.imshow(x_train[1,:,:], cmap='binary')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R3SkiZzEbach"},"source":["We can see our data has this shape:"]},{"cell_type":"code","metadata":{"id":"QLpKn1Skbaci"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M9tOvRcAbacj"},"source":["## Making a Neural Network"]},{"cell_type":"markdown","metadata":{"id":"-tOqjZFuback"},"source":["Let's make a simple neural network to classify the numbers here.  We'll make a convolutional neural network."]},{"cell_type":"code","metadata":{"id":"fOD5VjQVbacn"},"source":["model = keras.models.Sequential([\n","    keras.layers.Reshape((28, 28, 1), input_shape = (28,28)),\n","    keras.layers.Conv2D(2, (3, 3), activation = 'relu'),\n","    keras.layers.MaxPooling2D((2,2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(units = 30, activation = 'relu'),\n","    keras.layers.Dense(units = 10, activation = 'softmax'),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1YIP8Imrbacp"},"source":["The first layer here reshapes the data.  Each point in our data is a 2D grid of numbers (a 2D tensor), but convolutional layers want a 2D grid of *vectors* (a 3D tensor).  So we have the reshape layer fix this, adding a final dimension of length 1.  We assign an `input_shape` to the first layer so our model knows the expected size of the input data.  The `input_shape` of the other layers is not needed because it can be inferred by the previous."]},{"cell_type":"markdown","metadata":{"id":"hsPCk7tXbacq"},"source":["As we explained in our last meeting, let's make our validation set."]},{"cell_type":"code","metadata":{"id":"d3eLs-KRbacq"},"source":["from sklearn.model_selection import train_test_split\n","\n","x_train2, x_valid, y_train2, y_valid = train_test_split(x_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a32WM2vbbacr"},"source":["Then, we can build and train our model."]},{"cell_type":"code","metadata":{"id":"kVtZZGvPbacr"},"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","              metrics=['accuracy'])\n","\n","history = model.fit(x_train2, \n","                    y_train2,\n","                    batch_size=32,          # defaults to batch size of 32\n","                    epochs=5,\n","                    validation_data=(x_valid, y_valid))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KhkgEboXbacs"},"source":["The accuracy we get is somewhere around 0.93 on the training set and on the validation set.  The exact number will vary, since there's some randomness involved. We can see we did so good with such a simple convolutional network.\n","\n","Can we do even better?  The answer is, yes!\n","<br/><br/>\n","\n","For the next part of our meeting, take the time to poke around this model and see if you can produce a better one.\n","\n","Some advice on how to produce a better model:\n","- Try a bunch of things!  You'll never know what works!  Also, keep in mind that these can be *very* finicky.  Suppose you make one change, A, and see that it makes your model get worse.  You undo the change and try a different change, B, that also makes your model worse.  It's very possible that if you do A and B *together* you'll get a better than what you had!  It's weird that way.\n","- That being said, if you make a change and you do *better*, you should think about keeping it!  You might undo it a bit as you change more, but it's good to have the best model you've made so far as your current baseline.\n","- You might want to run your model multiple times when you change things!  That way you know you didn't just get lucky if it looked good or unlucky if it looked bad/the same.\n","\n","And here are a list of things you might want to change:\n","- How long the model trains (the number of epochs)\n","- How many layers are in the model\n","- The types of each layer (dense, convolutional, pooling, etc.)\n","- How big each hidden layer is (in terms of number of nodes or the number of output filters)\n","- The activation functions.  A list is [here](https://www.tensorflow.org/api_docs/python/tf/keras/activations).  All you need to do is write the name in quotes (e.g. activation = 'elu').  Don't worry if you don't understand the difference between them, it's not too important right now.\n","- The optimizer we use.  A list is [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).  Some of them might give you an error.  If so, just try a different one.  Again, it's okay if you don't understand how the different optimizers work.  You don't need to!  Again, you can just type the name in quotes (e.g., optimizer='sgd').\n","- Consider doing a little more pre-processing on the data.  There are tips on how to pre-process data in the [worksheet](https://nbviewer.jupyter.org/urls/www.math.ucla.edu/~egeo/classes/spr21_pic16b-1/week5/ml_basics.ipynb) mentioned at the beginning.\n","\n","There are still other things you can try. While you're working, if you want more ideas, you can ask me or you can look through the [Tensorflow keras API](https://www.tensorflow.org/api_docs/python/tf/keras/).\n","\n","Lastly, as a goal to aim towards: 96% accuracy or more is possible with just the changes above.  If you get there, see if you can do better!"]}]}